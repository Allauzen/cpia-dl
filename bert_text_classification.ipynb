{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session, we will build a text classifier for sentiment analysis. The idea is to reuse a pre-trained BERT model and to fine-tune it with a small amount of data. \n",
    "\n",
    "\n",
    "# Introduction\n",
    "The hugging face python libraries provide convenient API to deal with transformer models, like BERT, ALBERTA, GPT, ... .  To quote their website: Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. Transformers support framework interoperability between PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "Each model can have some peculiarities in their i/o. The Huggingface propose an API that helps you to build a relatively generic code. When you re-use a transformer model, you have to take care of some important steps: \n",
    "\n",
    "- What is required in input ? The simple sequence of indices is not enough in most of the case. \n",
    "- What is the output ? Do you use just a re-trained model, the model was trained using a self-supervised  criterion like masked language modeling. Or do you rely on an already fine-tuned model ? \n",
    "\n",
    "\n",
    "The roadmap: \n",
    "- Load the data\n",
    "- Use the tokenizer associated with your model\n",
    "- Build dataloaders\n",
    "- Fine tune BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee_x9Yq5hTuO",
    "outputId": "a9dbf1e3-8513-48aa-f22c-5cbcc3207883"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuNg51plR2GM",
    "outputId": "779222a4-a86b-4e12-87d6-812113d2eff5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, Subset,  RandomSampler\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import os, gzip, pickle\n",
    "\n",
    "# If the machine you run this on has a GPU available with CUDA installed,\n",
    "# use it. Using a GPU for learning often leads to huge speedups in training.\n",
    "# See https://developer.nvidia.com/cuda-downloads for installing CUDA\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkOsT1Bttqod"
   },
   "source": [
    "## Download the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA6soGbGR__h"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/imdb_reviews.csv'\n",
    "if not os.path.isfile(DATA_PATH):\n",
    "    gdd.download_file_from_google_drive(\n",
    "        file_id='1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',\n",
    "        dest_path=DATA_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sUff5oag4SP"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv4YSE9Zg4SP",
    "outputId": "8b6e2bb9-e8a9-4871-bae2-73d821a5c6fb"
   },
   "outputs": [],
   "source": [
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psf1GHmTg4SQ",
    "outputId": "9476130f-0c46-4ff1-bff7-388080e8b05b"
   },
   "outputs": [],
   "source": [
    "alltexts = df.review.values\n",
    "alllabels = df.label.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some input texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alltexts[0])\n",
    "print(\"---------- label: \",alllabels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for BERT finetuning\n",
    "\n",
    "The input format to BERT looks like it is  \"over-specified\", especially if you focus on just one type of task (sequence classification, word tagging, paraphrase detection, ...). The format is thought to be \"generic\": \n",
    "- Add special tokens to the start and end of each sentence.\n",
    "- Pad & truncate all sentences to a single constant length.\n",
    "- Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
    "\n",
    "It looks like that: \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
    "\n",
    "## Tokenizer \n",
    "If you don't want to recreate this kind of inputs with your own hands, you can use the pre-trained tokenizer associated to BERT. Moreover the function `encode_plus` will:\n",
    "- Tokenize the sentence.\n",
    "- Prepend the `[CLS]` token to the start.\n",
    "- Append the `[SEP]` token to the end.\n",
    "- Map tokens to their IDs.\n",
    "- Pad or truncate the sentence to `max_length`\n",
    "- Create attention masks for `[PAD]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0m0rv3ug4SR"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful steps: \n",
    "message = \"hello my name is Kevin\"\n",
    "tok = tokenizer.tokenize(message)\n",
    "print(\"tokenized : \",tok)\n",
    "enc = tokenizer.encode(tok)\n",
    "print(\"encoded : \",enc)\n",
    "enc2 = tokenizer.encode_plus(tok)\n",
    "print(\"encoded plus : \",enc2)\n",
    "dec = tokenizer.decode(enc)\n",
    "print(\"decoded : \",dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENq6RXGSg4SS"
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        truncation=True,\n",
    "                        max_length = 20,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the important difference in the output between `encode` and `encode_plus`. For `encode_plus`, the output has the form of a python dictionnary with all you need for BERT.  \n",
    "- can you explain the outputs ? \n",
    "- do you know how to acces to each of them ? \n",
    "\n",
    "Now we can tokenize everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23zL5kpig4SU",
    "outputId": "3ad4427b-4188-4f6a-ed31-aa7733cb97e0"
   },
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "ntrain = 5000\n",
    "nvalid = 1000\n",
    "\n",
    "ids = np.arange(len(alllabels))\n",
    "np.random.shuffle(ids)\n",
    "trainidx = ids[:ntrain]\n",
    "valididx = ids[ntrain:ntrain+nvalid]\n",
    "allidx = ids[:ntrain+nvalid]\n",
    "for sent in tqdm(alltexts[allidx]):\n",
    "    encoding_dict = preprocessing(sent, tokenizer)\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "\n",
    "token_id = th.cat(token_id, dim = 0)\n",
    "attention_masks = th.cat(attention_masks, dim = 0)\n",
    "\n",
    "print(token_id.shape, token_id[:ntrain].shape)\n",
    "labels = th.tensor(alllabels[allidx])\n",
    "print(\"\\n\", labels.sum().item(), \"postive labels / \", labels.shape[0], \"total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader\n",
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. \n",
    "\n",
    "PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAeqiStSg4SY"
   },
   "outputs": [],
   "source": [
    "train_set = TensorDataset(token_id[:ntrain], \n",
    "                          attention_masks[:ntrain], \n",
    "                          labels[:ntrain])\n",
    "\n",
    "valid_set = TensorDataset(token_id[ntrain:ntrain+nvalid], \n",
    "                          attention_masks[ntrain:ntrain+nvalid], \n",
    "                          labels[ntrain:ntrain+nvalid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVuKFSa9g4SY"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "valid_dataloader = DataLoader(\n",
    "            valid_set,\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT fine-Tuning\n",
    "\n",
    "For this task, we want to start from a pre-trained BERT model. If it was pre-trained as a MLM, we need to modify the architecture to get outputs for classification. Then we want to resume the training process (this is fine-tuning)  on our dataset. The final goal is to get a model which is well-suited for our task. \n",
    "\n",
    "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
    "\n",
    "Here is the current list of classes provided for fine-tuning:\n",
    "* BertModel\n",
    "* BertForPreTraining\n",
    "* BertForMaskedLM\n",
    "* BertForNextSentencePrediction\n",
    "* **BertForSequenceClassification** - The one we'll use.\n",
    "* BertForTokenClassification\n",
    "* BertForQuestionAnswering\n",
    "\n",
    "The documentation for these can be found under [here](https://huggingface.co/transformers/v4.7.0/model_doc/bert.html).\n",
    "\n",
    "\n",
    "We'll be using [BertForSequenceClassification](https://huggingface.co/transformers/v4.7.0/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. \n",
    "\n",
    "There are different pre-trained BERT models available. `bert-base-uncased` means the version that has only lowercase letters (\"uncased\") and is the smaller version (\"base\" vs \"large\").\n",
    "\n",
    "The documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v4.7.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), with the additional parameters defined [here](https://huggingface.co/transformers/v4.7.0/main_classes/configuration.html#transformers.PretrainedConfig).\n",
    "\n",
    "## Loading BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "ae5866d19c4d4311a49e191952075a4a",
      "bc6ee2ff2a4542748ea5a76014a9e865",
      "62cb171b245142849180c85dd38dc15d",
      "8464a35ce671475481b055c993fc3c91",
      "79e3794609e5436eb9730ef268e0d2d1",
      "a6911789959d435a95418d4734c04d3d",
      "359705b5458144d0a64045c3b6da6194",
      "85ac5246147440d4afc4836776ae4469",
      "5679a903189244dea191fc89c3970579",
      "404ffdbabf4c406ea4df799246feb228",
      "80d6a2c7068741668d3d1bf102da41a8"
     ]
    },
    "id": "dBSv5T3-g4SY",
    "outputId": "c045374a-8db2-41b9-ee00-b6512d1168ea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bmodel = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "\n",
    "print(bmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there might be a lot of questions: \n",
    "- what does the warning means ? \n",
    "- why `num_lables=2` ? \n",
    "- and the other options ? \n",
    "- and the print ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for curiosity's sake, we can browse all of the model's parameters by name here.\n",
    "\n",
    "In the below cell, I've printed out the names and dimensions of the weights for:\n",
    "\n",
    "- The embedding layer\n",
    "- The first of the twelve transformers\n",
    "- The output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(bmodel.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your bert \n",
    "We can already try the model on the validation set. Before just look at the output of the model on one batch. \n",
    "- interpret the output !  \n",
    "- do you understand everything ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dloader): \n",
    "    \"\"\"Run the BERT model for text classification and compute \n",
    "    the loss as well as the accuracy\"\"\"\n",
    "    ## TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and test the function \n",
    "validation(bmodel, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think about the result ? \n",
    "\n",
    "\n",
    "## Fine-Tuning \n",
    "\n",
    "With our model loaded and ready,  we need to grab the training hyperparameters from within the stored model. For fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
    "\n",
    "- **Batch size:** 16, 32  \n",
    "- **Learning rate (Adam):** 5e-5, 3e-5, 2e-5  \n",
    "- **Number of epochs:** 2, 3, 4 \n",
    "\n",
    "We chose:\n",
    "* Batch size: 32 (set when creating our DataLoaders)\n",
    "* Learning rate: 5e-5\n",
    "* Epochs: 5 (we'll see that this is probably too many...)\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHQWMQBXY-vy"
   },
   "outputs": [],
   "source": [
    "## Fine tune BERT: \n",
    "## TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT \n",
    "\n",
    "In this part we will see another use-case: \n",
    "- We have a pretrained model, but we don't have the adapted architecture for text classification. \n",
    "- As an illustration, we will use DistilBERT as another BERT-like model. It is lighter yet very efficient. [Here is the paper](https://arxiv.org/abs/1910.01108) that describes the distillation. \n",
    "\n",
    "The idea is to use a Wrapper around DistilBERT. \n",
    "\n",
    "##  Tokenizer and dataloader\n",
    "\n",
    "As before, you have to create what you need for the interface. Maybe there are some differences in the i/o, but it is easy. \n",
    "The roadmap:\n",
    "- get the tokenizer\n",
    "- process the data\n",
    "- create the dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txPs1kSBk1dw"
   },
   "outputs": [],
   "source": [
    "## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wrapper\n",
    "\n",
    "We can simply write a class that load the pretrained DistilBert model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_TBZnE0k3Zv"
   },
   "outputs": [],
   "source": [
    "class DistilBertClassifier(th.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.distilbert = # TODO \n",
    "        # Add a Dropout layer and a Linear classifier\n",
    "        \n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHSSu_GQltoi",
    "outputId": "922fed94-b62c-43c4-b4c6-f33a375ee46b"
   },
   "outputs": [],
   "source": [
    "# Create a model and test it on a batch \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMhTCXNWlxvy"
   },
   "source": [
    "Since we have created the model with almost our own hands, we have to define the loss function. Write a function that compute the validation scores (loss and accuracy) with your model and a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksxTxtpLbvR4",
    "outputId": "a117c6f9-8426-4eef-bda7-92ccdea073e1"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCBbz6SfmDje",
    "outputId": "f56cdbf0-15ab-46d3-98a9-284cadd821d4"
   },
   "outputs": [],
   "source": [
    "def validDB(model, loader):\n",
    "  ## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning \n",
    "\n",
    "Lets go: train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "K0rYJcJ_Xnhq",
    "outputId": "ec32920b-70d7-4458-e301-cc2b843057d1"
   },
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZftp0VOf4mq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "359705b5458144d0a64045c3b6da6194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "404ffdbabf4c406ea4df799246feb228": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5679a903189244dea191fc89c3970579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "62cb171b245142849180c85dd38dc15d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85ac5246147440d4afc4836776ae4469",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5679a903189244dea191fc89c3970579",
      "value": 440449768
     }
    },
    "79e3794609e5436eb9730ef268e0d2d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80d6a2c7068741668d3d1bf102da41a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8464a35ce671475481b055c993fc3c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_404ffdbabf4c406ea4df799246feb228",
      "placeholder": "​",
      "style": "IPY_MODEL_80d6a2c7068741668d3d1bf102da41a8",
      "value": " 440M/440M [00:05&lt;00:00, 87.2MB/s]"
     }
    },
    "85ac5246147440d4afc4836776ae4469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6911789959d435a95418d4734c04d3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae5866d19c4d4311a49e191952075a4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc6ee2ff2a4542748ea5a76014a9e865",
       "IPY_MODEL_62cb171b245142849180c85dd38dc15d",
       "IPY_MODEL_8464a35ce671475481b055c993fc3c91"
      ],
      "layout": "IPY_MODEL_79e3794609e5436eb9730ef268e0d2d1"
     }
    },
    "bc6ee2ff2a4542748ea5a76014a9e865": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6911789959d435a95418d4734c04d3d",
      "placeholder": "​",
      "style": "IPY_MODEL_359705b5458144d0a64045c3b6da6194",
      "value": "Downloading model.safetensors: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
