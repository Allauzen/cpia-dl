{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab session in an introduction to feed-forward and convolutional neural network with pytorch. We use the dataset Fashion-MNIST (see for more details this website https://github.com/zalandoresearch/fashion-mnist). The dataset contains 60000 and 10000 images for respectively training and testing. Each image is 28x28 pixels, for a total of 784 per image.  An image is presented to the neural network as a flat vector of 784 component. \n",
    "\n",
    "First load and test python and pytorch. Your notebook is supposed to work with python 3 (see the top right corner of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allauzen/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/allauzen/anaconda3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# math, numpy and plot\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "# torch \n",
    "import torch as th\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# gzip \n",
    "import gzip\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset for the training/validation sets\n",
    "valid_ratio = 0.3\n",
    "dataset_dir = \"./\"\n",
    "train_valid_dataset = datasets.FashionMNIST(root=dataset_dir,\n",
    "                                           train=True,\n",
    "                                           transform= ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "# Split it into training and validation sets\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "train_dataset, valid_dataset = th.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In pytorch data handling is done in 2 steps: \n",
    "- `DataSet`: a class to access the raw data, it can be tensors, files, distributed files, ... \n",
    "- `DataLoader` (the class to iterate through the dataset and to get access to well prepared batch of data)\n",
    "\n",
    "From the model viewpoint: \n",
    "- During training and testing the model interacts with the `DataLoader` to go through the `DataSet`\n",
    "- The Dataloader pick what is necessary in the `DataSet`  \n",
    "\n",
    "Here we already have dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at one batch : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =500\n",
    "trainloader =  DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([500, 1, 28, 28]) torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(trainloader))\n",
    "# Explore what you get as a batch\n",
    "print(type(batch),len(batch))\n",
    "print(type(batch[0]),type(batch[1]))\n",
    "print(batch[0].shape,batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 1\n",
    "image = batch[0][i]\n",
    "label = batch[1][i]\n",
    "plt.imshow(image.squeeze().numpy() , matplotlib.pyplot.cm.gray)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt\n"
     ]
    }
   ],
   "source": [
    "print(labels_map[label.item() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward neural network\n",
    "\n",
    "\n",
    "\n",
    "A feedforward model can rely on the pytorch module *Sequential*. \n",
    "A *Sequential* module is a container of ordered modules: the first one takes input data and its output is given to feed the second module, and so on. \n",
    "\n",
    "**Note:** In pytorch, modules assumed Tensors as input. The input Tensor can contain just one input (online mode) or several inputs (mini-batch). The first dimension of the input Tensor corresponds to the mini-batch, and the second one to the dimension of the example to feed in. For example, with a mini-batch of size B and an image of D pixels, the input Tensor should be of shape (B,D), even if B=1 for one exemple. \n",
    "\n",
    "\n",
    "## Shallow network\n",
    "\n",
    "Let start with a simple model with one input layer and one output layer (without hidden layers). Please refer to the examples provided previously, and propose an implementation of this linear model using the *Sequential* module as container.  To write the model, we must consider the fact that the model is trained in order to maximize the Log-likelihood on the training data. If you look at  https://pytorch.org/docs/stable/nn.html, the documentation of the NNet package of pytorch, there is a section on the loss functions. \n",
    "\n",
    "Two of the proposed loss function can be used for our purpose. The choice of one of them implies the choice of the activation function at the output layer. \n",
    "\n",
    "- What are these two possible choices ? \n",
    "- Make a choice and replace the \"None\" in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([ -51.5904, -122.3245, -212.5135, -160.3159,  -31.0036, -272.6298,\n",
      "        -179.6013,    0.0000, -196.8257,  -70.1345], grad_fn=<SelectBackward0>)\n",
      "tensor(160.3159, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "D_in = 784\n",
    "D_out= 10\n",
    "\n",
    "## TODO : replace \"None\"\n",
    "model =  nn.Sequential(\n",
    "    nn.Linear(D_in,D_out),\n",
    "    nn.LogSoftmax(dim=1)    \n",
    ")\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "image = image.view(1,-1)\n",
    "\n",
    "preds = model(image)\n",
    "print(preds.shape)\n",
    "print(preds[0])\n",
    "loss = loss_function(preds.squeeze(), label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you explain what we observe ? \n",
    "- Do it the same for one batch and explain what you observe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the network on the training data and evaluate what happens on developpment data. \n",
    "- Play with the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP training \n",
    "\n",
    "- Build and train a MLP with one hidden layer. Look at the performance. \n",
    "- Play with the learning rate. \n",
    "- Can you estimate the number of parameter and how does it scale with dimensions (image, number of classes, ...)\n",
    "- Then try to add one more hidden layer. \n",
    "- And one more ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution \n",
    "\n",
    "The goal is to incrementally build an image classifier based on convolutional layers. Since we consider images and convolution we will use Tensors with peculiar shapes in input. Moreover, this session is also the occasion to introduce \"Max-pooling\" and \"Batch-normalization\". \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "First the dataset !  The provided version is unfortunately not adapted to our purpose in terms of dimensions. \n",
    "*Convolution2D* expects as input a Tensor with 4 dimensions $(N,C,H,W)$ with : \n",
    "\n",
    "- N the batch dimension, *i.e* the number of images\n",
    "- C the number of input channels, here it is 1\n",
    "- H the height or number of rows of each image\n",
    "- W the width  or number of columns of each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with convolution in 2D \n",
    "\n",
    "Let start the exploration of convolution.The class we will use is called Conv2d. Read carefully the documentation of this module. Maybe you cannot understand everything. That's why it is useful to first play with the convolution with one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract one image to start and be sure you have the right dimensions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create one convolution layer with 1 input channels, a kernel size of 3, and a stride of 1. \n",
    "Try it and look at the output dimension. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting \"image\" is not of the same dimension, how to obtain an output with the same dimension (same question with a kernel size of 5) ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of \"handmaid\" filters\n",
    "\n",
    "We can define the parameters of the convolutional filter with our own hands. For that purpose we just have to create the Tensor we want and cast it in a *Parameter* object (usefull for autograd) and then assign it. \n",
    "This is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a convolutional filter\n",
    "convFilter = nn.Conv2d(in_channels=1,out_channels=1,\n",
    "                       kernel_size = 3, padding=1,\n",
    "                       stride=1)\n",
    "# build the weight matrix you want \n",
    "W=th.ones(convFilter.weight.shape) \n",
    "# Makes it a Parameter and assign\n",
    "convFilter.weight = nn.Parameter(W)\n",
    "\n",
    "res = convFilter(im)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im.squeeze(),  matplotlib.pyplot.cm.gray)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(F.relu(res).squeeze().detach(), matplotlib.pyplot.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to set the convolution fiter as follows:\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    " -1 &2&-1\\\\\n",
    " -1 &2&-1\\\\\n",
    " -1 &2&-1\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "and then as follows: \n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{rrr}\n",
    " -1 &-1&-1\\\\\n",
    " 2 &2&2\\\\\n",
    " -1 &-1&-1\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "Try them on some images and visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool ! \n",
    "\n",
    "Now we introduce the max-pooling in 2 dimensions: *MaxPool2d*. Look at the documentation and then try to define the following pipeline: \n",
    "- a convolution with a kernel size of 3, stride 1 and padding 1\n",
    "- apply the ReLu function and\n",
    "- a max pooling with kernel size of 2 and a stride of 2. \n",
    "Try to guess before running your code the dimensions of the output ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A first model\n",
    "\n",
    "After this interlude, the goal now is to write a class to implement the model with:\n",
    "\n",
    "- 2D convolution with (kernel size = 3, padding = 1, stride 1)\n",
    "- ReLu activation \n",
    "- Max-pooling (kernel size = 2, stride 2)\n",
    "- A final linear classifier\n",
    "- The final activation\n",
    "\n",
    "Writing this class, allows you to wrap what you have seen so far. To debug the model, you can first play step-by-step with each layer to ensure you obtain the right dimensions (it was done earlier). Then, write the class and run the training to evaluate the result (this what we have to do now).\n",
    "\n",
    "The class inherits from an existing class of pytorch : *Module*. This mean: it is a *Module*, but we add some peculiarities. For that purpose we can fill the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class FashionCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size = 3, padding= 1, out_channels = 1):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        # TODO : write the end of the constructor.\n",
    "        # It is important to create here all the layers of the network. \n",
    "        # All layers that have paramaters should be attribute. \n",
    "        # For example: \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, \n",
    "                              kernel_size=kernel_size, padding=padding)\n",
    "        # TODO: add the rest\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        # if you need to run forward with the conv layer, \n",
    "        # you can call it by self.conv \n",
    "        outconv  = self.conv(x)\n",
    "        # ... \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the class: is everything in place:\n",
    "# A first classifier is built like : \n",
    "classif = FashionCNN()\n",
    "# The parameters of the classifier are randomly initialize, but we \n",
    "# can use it on a image : \n",
    "out = classif.forward(im)\n",
    "print(out.shape) # the output has 2 dimensions \n",
    "print(out)\n",
    "\n",
    "# It is correct ? If not, correct the class to get the expected result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "To train the model, we need to define a loss function and an optimizer. For the moment we will rely on an online learning algorithm: online stochastic gradient descent. Like the previous lab session: \n",
    "- we pick one training example\n",
    "- compute the loss\n",
    "- back-propagation of the gradient \n",
    "- update of the parameters\n",
    "\n",
    "\n",
    "At the end of one epoch, we evaluate the model on the validation step. You can use for that purpose the training function we wrote earlier. \n",
    "\n",
    "\n",
    "To train the CNN, we can reuse the training function you wrote carefully in the previous lab session. However we need to adapt it in order to use dataloaders (the `trainloader` and `validloader`) \n",
    "\n",
    "Question: \n",
    "- As optimizer we will use *Adam*. It is important to find the good choice of hyper-parameter for the initial learning rate. Try different values like 0.1, 0.01, ... \n",
    "- Then try with a number of output channel set to 1, 8, 16. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : paste here the training function you wrote before.\n",
    "# Of course you can improve it if you want during the lab session.\n",
    "# Especially in terms of parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "loss_fn=nn.NLLLoss()\n",
    "model = FashionCNN()\n",
    "# Train you model ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Batch-norm\n",
    "\n",
    "Extend your model to include the Batch-normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More convolution\n",
    "\n",
    "We can now define an extended model where the basic block is : Conv2D, ReLu, BatchNorm and MaxPool. \n",
    "We stack two blocks of this kind before the classification. \n",
    "\n",
    "For instance in the previous model, this kind of block reduce the image size and increase the number of output channels. We can try to do the same and double this number in the second block. \n",
    "\n",
    "TODO: \n",
    "- Implement a model with two blocks as decribed above. \n",
    "- We can then improve the output classifier.\n",
    "- Play with the hyper-parameters.\n",
    "\n",
    "Of course if you want to leverage a deeper model it is useful to increase the amount of training data (we only take the first 30k images until now). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now in color: CIFAR-10\n",
    "\n",
    "\n",
    "To experiment image classification on a coloured image, we can use the CIFAR-10 dataset. \n",
    "You can find more details for instance on this page: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html. We can download the dataset with a dataloader directly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = th.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = th.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this example we will use a **dataset** via a *dataloader*.  This is a convenient tool to handle datasets with efficient iterators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo \n",
    "\n",
    "Implement `VGG 16` architecture to get state of the art performance (see the course for the architecture) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
